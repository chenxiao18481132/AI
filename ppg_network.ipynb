{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ppg  network.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vb_05yw7kG6R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "f01dc148-c166-44a9-fb6d-74f829730bae"
      },
      "source": [
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "#X_train\n",
        "from sklearn.model_selection import train_test_split\n",
        "# prepocessing data\n",
        "yy=pd.read_csv('ydata.csv')\n",
        "x=pd.read_csv('xdata.csv')\n",
        "x=x.iloc[:,1:]\n",
        "x = preprocessing.scale(x)\n",
        "label=yy.iloc[:,3]\n",
        "label=np.array(label)\n",
        "label\n",
        "label=label.reshape(label.shape[0],1)\n",
        "print(label.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10113, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by the scale function.\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mP2kc0phlYYV"
      },
      "source": [
        "X_tr, X_vld, lab_tr, lab_vld = train_test_split(x, label, \n",
        "                                                 random_state = 123)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDzsdK7Tl81o"
      },
      "source": [
        "import tensorflow as tf\n",
        "lr=0.001#learning rate\n",
        "epochs=6000\n",
        "w=4 # parameter for weighted loss\n",
        "inputs_ = tf.placeholder(tf.float32, [None,47])     # input x\n",
        "labels_ = tf.placeholder(tf.float32, [None,1])     # input y\n",
        "\n",
        "# neural network layers\n",
        "l1 = tf.layers.dense(inputs_, 16, tf.nn.relu)#25,10 ->249.14749\n",
        "l2 = tf.layers.dense(l1, 8, tf.nn.relu)\n",
        "logits=tf.layers.dense(l2,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXzAjD7UoT8d"
      },
      "source": [
        "prediction=tf.nn.sigmoid(logits)\n",
        "\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(targets=labels_,logits=logits,pos_weight=w))#4 classes\n",
        "global_step = tf.Variable(0)\n",
        "\n",
        "learning_rate = tf.train.exponential_decay(lr, global_step, 50, 0.96, staircase=True) \n",
        "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost,global_step=global_step)\n",
        "\n",
        "# Calculate the correct predictions\n",
        "correct_prediction = tf.to_float(tf.greater(prediction, 0.5))\n",
        "accuracy = tf.reduce_mean(tf.to_float(tf.equal(labels_, correct_prediction)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Vu4vXgBMlop"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2xFqjoeo4SV"
      },
      "source": [
        "#  accuaracy of positive examples and negtive examples\n",
        "accc=tf.equal(labels_, correct_prediction)\n",
        "N1=tf.reduce_sum(labels_)\n",
        "N=tf.reduce_sum(tf.ones_like(labels_))\n",
        "N0=N-N1\n",
        "mask_1=tf.equal(labels_,tf.ones_like(labels_))\n",
        "accp=tf.reduce_mean(tf.to_float(tf.boolean_mask(accc, mask_1)))\n",
        "mask_0=tf.equal(labels_,tf.zeros_like(labels_))\n",
        "accn=tf.reduce_mean(tf.to_float(tf.boolean_mask(accc, mask_0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "674IEpLao-Mu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2057
        },
        "outputId": "5871232a-280f-4d84-b388-0f3a818ae542"
      },
      "source": [
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "   \n",
        "    # Loop over epochs\n",
        "    for e in range(epochs):\n",
        "\n",
        "            train,_,_,_=sess.run([accuracy, optimizer,learning_rate,global_step], feed_dict={inputs_ :  X_tr, labels_: lab_tr})\n",
        "                \n",
        "            #, feature_:featuretrain\n",
        "            # Print at each 5 iters\n",
        "            if (e % 50 == 0):\n",
        "                \n",
        "                #acc=sess.run(accuracy, feed_dict={inputs_ : X_train, labels_: label, feature_: feature, tf_is_training: False})\n",
        "                acp,acn,acctest=sess.run([accp,accn,accuracy], feed_dict={inputs_ : X_vld, labels_: lab_vld})\n",
        "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
        "                      \"Train loss: {:6f}\".format(train),\n",
        "                      \"Test acc: {:.6f}\".format(acctest),\n",
        "                      \"Test accp: {:.6f}\".format(acp),\n",
        "                      \"Test accn: {:.6f}\".format(acn))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0/6000 Train loss: 0.417194 Test acc: 0.412021 Test accp: 0.552000 Test accn: 0.366071\n",
            "Epoch: 50/6000 Train loss: 0.607068 Test acc: 0.607750 Test accp: 0.811200 Test accn: 0.540966\n",
            "Epoch: 100/6000 Train loss: 0.662975 Test acc: 0.675761 Test accp: 0.808000 Test accn: 0.632353\n",
            "Epoch: 150/6000 Train loss: 0.688555 Test acc: 0.693159 Test accp: 0.800000 Test accn: 0.658088\n",
            "Epoch: 200/6000 Train loss: 0.701609 Test acc: 0.705022 Test accp: 0.800000 Test accn: 0.673845\n",
            "Epoch: 250/6000 Train loss: 0.707410 Test acc: 0.705813 Test accp: 0.792000 Test accn: 0.677521\n",
            "Epoch: 300/6000 Train loss: 0.715058 Test acc: 0.711348 Test accp: 0.793600 Test accn: 0.684349\n",
            "Epoch: 350/6000 Train loss: 0.719541 Test acc: 0.715302 Test accp: 0.795200 Test accn: 0.689076\n",
            "Epoch: 400/6000 Train loss: 0.723761 Test acc: 0.717675 Test accp: 0.796800 Test accn: 0.691702\n",
            "Epoch: 450/6000 Train loss: 0.724420 Test acc: 0.723211 Test accp: 0.796800 Test accn: 0.699055\n",
            "Epoch: 500/6000 Train loss: 0.725475 Test acc: 0.728747 Test accp: 0.800000 Test accn: 0.705357\n",
            "Epoch: 550/6000 Train loss: 0.728903 Test acc: 0.733887 Test accp: 0.804800 Test accn: 0.710609\n",
            "Epoch: 600/6000 Train loss: 0.731936 Test acc: 0.734678 Test accp: 0.801600 Test accn: 0.712710\n",
            "Epoch: 650/6000 Train loss: 0.734045 Test acc: 0.735469 Test accp: 0.803200 Test accn: 0.713235\n",
            "Epoch: 700/6000 Train loss: 0.734705 Test acc: 0.735469 Test accp: 0.804800 Test accn: 0.712710\n",
            "Epoch: 750/6000 Train loss: 0.737869 Test acc: 0.738632 Test accp: 0.809600 Test accn: 0.715336\n",
            "Epoch: 800/6000 Train loss: 0.740374 Test acc: 0.742586 Test accp: 0.811200 Test accn: 0.720063\n",
            "Epoch: 850/6000 Train loss: 0.740902 Test acc: 0.744958 Test accp: 0.811200 Test accn: 0.723214\n",
            "Epoch: 900/6000 Train loss: 0.742484 Test acc: 0.744958 Test accp: 0.806400 Test accn: 0.724790\n",
            "Epoch: 950/6000 Train loss: 0.743671 Test acc: 0.745354 Test accp: 0.798400 Test accn: 0.727941\n",
            "Epoch: 1000/6000 Train loss: 0.745253 Test acc: 0.746936 Test accp: 0.795200 Test accn: 0.731092\n",
            "Epoch: 1050/6000 Train loss: 0.745121 Test acc: 0.747331 Test accp: 0.793600 Test accn: 0.732143\n",
            "Epoch: 1100/6000 Train loss: 0.747890 Test acc: 0.748517 Test accp: 0.793600 Test accn: 0.733719\n",
            "Epoch: 1150/6000 Train loss: 0.749077 Test acc: 0.749703 Test accp: 0.795200 Test accn: 0.734769\n",
            "Epoch: 1200/6000 Train loss: 0.750264 Test acc: 0.750890 Test accp: 0.798400 Test accn: 0.735294\n",
            "Epoch: 1250/6000 Train loss: 0.750923 Test acc: 0.754053 Test accp: 0.798400 Test accn: 0.739496\n",
            "Epoch: 1300/6000 Train loss: 0.751714 Test acc: 0.754844 Test accp: 0.798400 Test accn: 0.740546\n",
            "Epoch: 1350/6000 Train loss: 0.752505 Test acc: 0.753262 Test accp: 0.796800 Test accn: 0.738971\n",
            "Epoch: 1400/6000 Train loss: 0.753692 Test acc: 0.754053 Test accp: 0.796800 Test accn: 0.740021\n",
            "Epoch: 1450/6000 Train loss: 0.754088 Test acc: 0.754053 Test accp: 0.795200 Test accn: 0.740546\n",
            "Epoch: 1500/6000 Train loss: 0.755011 Test acc: 0.756425 Test accp: 0.798400 Test accn: 0.742647\n",
            "Epoch: 1550/6000 Train loss: 0.755406 Test acc: 0.757612 Test accp: 0.798400 Test accn: 0.744223\n",
            "Epoch: 1600/6000 Train loss: 0.755934 Test acc: 0.758798 Test accp: 0.798400 Test accn: 0.745798\n",
            "Epoch: 1650/6000 Train loss: 0.755406 Test acc: 0.757612 Test accp: 0.796800 Test accn: 0.744748\n",
            "Epoch: 1700/6000 Train loss: 0.756197 Test acc: 0.758007 Test accp: 0.796800 Test accn: 0.745273\n",
            "Epoch: 1750/6000 Train loss: 0.756065 Test acc: 0.760380 Test accp: 0.798400 Test accn: 0.747899\n",
            "Epoch: 1800/6000 Train loss: 0.756329 Test acc: 0.760775 Test accp: 0.798400 Test accn: 0.748424\n",
            "Epoch: 1850/6000 Train loss: 0.756197 Test acc: 0.760775 Test accp: 0.800000 Test accn: 0.747899\n",
            "Epoch: 1900/6000 Train loss: 0.756461 Test acc: 0.759589 Test accp: 0.796800 Test accn: 0.747374\n",
            "Epoch: 1950/6000 Train loss: 0.758439 Test acc: 0.758798 Test accp: 0.793600 Test accn: 0.747374\n",
            "Epoch: 2000/6000 Train loss: 0.757911 Test acc: 0.758798 Test accp: 0.792000 Test accn: 0.747899\n",
            "Epoch: 2050/6000 Train loss: 0.758703 Test acc: 0.758007 Test accp: 0.792000 Test accn: 0.746849\n",
            "Epoch: 2100/6000 Train loss: 0.758703 Test acc: 0.756821 Test accp: 0.792000 Test accn: 0.745273\n",
            "Epoch: 2150/6000 Train loss: 0.759626 Test acc: 0.759193 Test accp: 0.796800 Test accn: 0.746849\n",
            "Epoch: 2200/6000 Train loss: 0.759889 Test acc: 0.760380 Test accp: 0.798400 Test accn: 0.747899\n",
            "Epoch: 2250/6000 Train loss: 0.759230 Test acc: 0.760775 Test accp: 0.801600 Test accn: 0.747374\n",
            "Epoch: 2300/6000 Train loss: 0.759230 Test acc: 0.759589 Test accp: 0.801600 Test accn: 0.745798\n",
            "Epoch: 2350/6000 Train loss: 0.760285 Test acc: 0.758403 Test accp: 0.800000 Test accn: 0.744748\n",
            "Epoch: 2400/6000 Train loss: 0.758571 Test acc: 0.758403 Test accp: 0.800000 Test accn: 0.744748\n",
            "Epoch: 2450/6000 Train loss: 0.757911 Test acc: 0.759984 Test accp: 0.801600 Test accn: 0.746324\n",
            "Epoch: 2500/6000 Train loss: 0.758703 Test acc: 0.760380 Test accp: 0.801600 Test accn: 0.746849\n",
            "Epoch: 2550/6000 Train loss: 0.760944 Test acc: 0.761170 Test accp: 0.801600 Test accn: 0.747899\n",
            "Epoch: 2600/6000 Train loss: 0.761208 Test acc: 0.761170 Test accp: 0.803200 Test accn: 0.747374\n",
            "Epoch: 2650/6000 Train loss: 0.761603 Test acc: 0.761961 Test accp: 0.803200 Test accn: 0.748424\n",
            "Epoch: 2700/6000 Train loss: 0.761603 Test acc: 0.761961 Test accp: 0.803200 Test accn: 0.748424\n",
            "Epoch: 2750/6000 Train loss: 0.761735 Test acc: 0.761961 Test accp: 0.803200 Test accn: 0.748424\n",
            "Epoch: 2800/6000 Train loss: 0.762658 Test acc: 0.761170 Test accp: 0.803200 Test accn: 0.747374\n",
            "Epoch: 2850/6000 Train loss: 0.762922 Test acc: 0.759193 Test accp: 0.801600 Test accn: 0.745273\n",
            "Epoch: 2900/6000 Train loss: 0.763318 Test acc: 0.759589 Test accp: 0.803200 Test accn: 0.745273\n",
            "Epoch: 2950/6000 Train loss: 0.763713 Test acc: 0.759984 Test accp: 0.804800 Test accn: 0.745273\n",
            "Epoch: 3000/6000 Train loss: 0.764109 Test acc: 0.759589 Test accp: 0.804800 Test accn: 0.744748\n",
            "Epoch: 3050/6000 Train loss: 0.763845 Test acc: 0.760775 Test accp: 0.804800 Test accn: 0.746324\n",
            "Epoch: 3100/6000 Train loss: 0.764241 Test acc: 0.759984 Test accp: 0.803200 Test accn: 0.745798\n",
            "Epoch: 3150/6000 Train loss: 0.763449 Test acc: 0.760380 Test accp: 0.806400 Test accn: 0.745273\n",
            "Epoch: 3200/6000 Train loss: 0.763186 Test acc: 0.761170 Test accp: 0.808000 Test accn: 0.745798\n",
            "Epoch: 3250/6000 Train loss: 0.763581 Test acc: 0.761170 Test accp: 0.808000 Test accn: 0.745798\n",
            "Epoch: 3300/6000 Train loss: 0.763713 Test acc: 0.761566 Test accp: 0.808000 Test accn: 0.746324\n",
            "Epoch: 3350/6000 Train loss: 0.763318 Test acc: 0.761566 Test accp: 0.808000 Test accn: 0.746324\n",
            "Epoch: 3400/6000 Train loss: 0.763186 Test acc: 0.761170 Test accp: 0.808000 Test accn: 0.745798\n",
            "Epoch: 3450/6000 Train loss: 0.763318 Test acc: 0.760775 Test accp: 0.806400 Test accn: 0.745798\n",
            "Epoch: 3500/6000 Train loss: 0.763318 Test acc: 0.761566 Test accp: 0.808000 Test accn: 0.746324\n",
            "Epoch: 3550/6000 Train loss: 0.763581 Test acc: 0.761566 Test accp: 0.806400 Test accn: 0.746849\n",
            "Epoch: 3600/6000 Train loss: 0.763449 Test acc: 0.761566 Test accp: 0.804800 Test accn: 0.747374\n",
            "Epoch: 3650/6000 Train loss: 0.763318 Test acc: 0.761170 Test accp: 0.803200 Test accn: 0.747374\n",
            "Epoch: 3700/6000 Train loss: 0.763318 Test acc: 0.761170 Test accp: 0.803200 Test accn: 0.747374\n",
            "Epoch: 3750/6000 Train loss: 0.763713 Test acc: 0.761170 Test accp: 0.803200 Test accn: 0.747374\n",
            "Epoch: 3800/6000 Train loss: 0.763581 Test acc: 0.761170 Test accp: 0.803200 Test accn: 0.747374\n",
            "Epoch: 3850/6000 Train loss: 0.764109 Test acc: 0.761170 Test accp: 0.803200 Test accn: 0.747374\n",
            "Epoch: 3900/6000 Train loss: 0.764109 Test acc: 0.760775 Test accp: 0.801600 Test accn: 0.747374\n",
            "Epoch: 3950/6000 Train loss: 0.763977 Test acc: 0.761170 Test accp: 0.801600 Test accn: 0.747899\n",
            "Epoch: 4000/6000 Train loss: 0.764372 Test acc: 0.761170 Test accp: 0.801600 Test accn: 0.747899\n",
            "Epoch: 4050/6000 Train loss: 0.764109 Test acc: 0.761170 Test accp: 0.801600 Test accn: 0.747899\n",
            "Epoch: 4100/6000 Train loss: 0.764372 Test acc: 0.760775 Test accp: 0.800000 Test accn: 0.747899\n",
            "Epoch: 4150/6000 Train loss: 0.764372 Test acc: 0.760380 Test accp: 0.798400 Test accn: 0.747899\n",
            "Epoch: 4200/6000 Train loss: 0.764372 Test acc: 0.761961 Test accp: 0.803200 Test accn: 0.748424\n",
            "Epoch: 4250/6000 Train loss: 0.764372 Test acc: 0.761961 Test accp: 0.803200 Test accn: 0.748424\n",
            "Epoch: 4300/6000 Train loss: 0.764372 Test acc: 0.761170 Test accp: 0.803200 Test accn: 0.747374\n",
            "Epoch: 4350/6000 Train loss: 0.764241 Test acc: 0.761170 Test accp: 0.803200 Test accn: 0.747374\n",
            "Epoch: 4400/6000 Train loss: 0.764109 Test acc: 0.761170 Test accp: 0.803200 Test accn: 0.747374\n",
            "Epoch: 4450/6000 Train loss: 0.764504 Test acc: 0.761566 Test accp: 0.803200 Test accn: 0.747899\n",
            "Epoch: 4500/6000 Train loss: 0.764636 Test acc: 0.761566 Test accp: 0.804800 Test accn: 0.747374\n",
            "Epoch: 4550/6000 Train loss: 0.764768 Test acc: 0.761566 Test accp: 0.804800 Test accn: 0.747374\n",
            "Epoch: 4600/6000 Train loss: 0.765032 Test acc: 0.761566 Test accp: 0.804800 Test accn: 0.747374\n",
            "Epoch: 4650/6000 Train loss: 0.765032 Test acc: 0.760775 Test accp: 0.803200 Test accn: 0.746849\n",
            "Epoch: 4700/6000 Train loss: 0.764768 Test acc: 0.760775 Test accp: 0.803200 Test accn: 0.746849\n",
            "Epoch: 4750/6000 Train loss: 0.765032 Test acc: 0.760775 Test accp: 0.803200 Test accn: 0.746849\n",
            "Epoch: 4800/6000 Train loss: 0.764636 Test acc: 0.760775 Test accp: 0.803200 Test accn: 0.746849\n",
            "Epoch: 4850/6000 Train loss: 0.764636 Test acc: 0.760775 Test accp: 0.803200 Test accn: 0.746849\n",
            "Epoch: 4900/6000 Train loss: 0.764900 Test acc: 0.761170 Test accp: 0.806400 Test accn: 0.746324\n",
            "Epoch: 4950/6000 Train loss: 0.764900 Test acc: 0.761566 Test accp: 0.804800 Test accn: 0.747374\n",
            "Epoch: 5000/6000 Train loss: 0.764636 Test acc: 0.761170 Test accp: 0.804800 Test accn: 0.746849\n",
            "Epoch: 5050/6000 Train loss: 0.764768 Test acc: 0.761961 Test accp: 0.806400 Test accn: 0.747374\n",
            "Epoch: 5100/6000 Train loss: 0.764768 Test acc: 0.761566 Test accp: 0.806400 Test accn: 0.746849\n",
            "Epoch: 5150/6000 Train loss: 0.764900 Test acc: 0.760775 Test accp: 0.804800 Test accn: 0.746324\n",
            "Epoch: 5200/6000 Train loss: 0.764636 Test acc: 0.760380 Test accp: 0.803200 Test accn: 0.746324\n",
            "Epoch: 5250/6000 Train loss: 0.764900 Test acc: 0.759984 Test accp: 0.801600 Test accn: 0.746324\n",
            "Epoch: 5300/6000 Train loss: 0.764504 Test acc: 0.759984 Test accp: 0.801600 Test accn: 0.746324\n",
            "Epoch: 5350/6000 Train loss: 0.764504 Test acc: 0.760380 Test accp: 0.801600 Test accn: 0.746849\n",
            "Epoch: 5400/6000 Train loss: 0.764768 Test acc: 0.760380 Test accp: 0.801600 Test accn: 0.746849\n",
            "Epoch: 5450/6000 Train loss: 0.764900 Test acc: 0.760380 Test accp: 0.801600 Test accn: 0.746849\n",
            "Epoch: 5500/6000 Train loss: 0.764900 Test acc: 0.760775 Test accp: 0.801600 Test accn: 0.747374\n",
            "Epoch: 5550/6000 Train loss: 0.765032 Test acc: 0.760775 Test accp: 0.801600 Test accn: 0.747374\n",
            "Epoch: 5600/6000 Train loss: 0.764900 Test acc: 0.760775 Test accp: 0.801600 Test accn: 0.747374\n",
            "Epoch: 5650/6000 Train loss: 0.765295 Test acc: 0.760380 Test accp: 0.801600 Test accn: 0.746849\n",
            "Epoch: 5700/6000 Train loss: 0.764900 Test acc: 0.760380 Test accp: 0.801600 Test accn: 0.746849\n",
            "Epoch: 5750/6000 Train loss: 0.765163 Test acc: 0.760775 Test accp: 0.801600 Test accn: 0.747374\n",
            "Epoch: 5800/6000 Train loss: 0.765295 Test acc: 0.760775 Test accp: 0.801600 Test accn: 0.747374\n",
            "Epoch: 5850/6000 Train loss: 0.765032 Test acc: 0.760775 Test accp: 0.801600 Test accn: 0.747374\n",
            "Epoch: 5900/6000 Train loss: 0.765032 Test acc: 0.760775 Test accp: 0.801600 Test accn: 0.747374\n",
            "Epoch: 5950/6000 Train loss: 0.764900 Test acc: 0.760775 Test accp: 0.801600 Test accn: 0.747374\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ruf7uGmJEqUq"
      },
      "source": [
        "def ROC(T):\n",
        "  \n",
        "  cost = tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(targets=labels_,logits=logits,pos_weight=T))\n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost,global_step=global_step)\n",
        "  with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "   \n",
        "    # Loop over epochs\n",
        "    for e in range(epochs):\n",
        "        \n",
        "     \n",
        "            train,_,_,_=sess.run([accuracy, optimizer,learning_rate,global_step], feed_dict={inputs_ :  X_tr, labels_: lab_tr})\n",
        "                \n",
        "           \n",
        "                \n",
        "              \n",
        "    acp,acn,acctest=sess.run([accp,accn,accuracy], feed_dict={inputs_ : X_vld, labels_: lab_vld})\n",
        "  return [acp,acn]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5aumj_0DSY8"
      },
      "source": [
        "h_test_list=[]# POSITIVE ACCUARACY\n",
        "n_test_list=[]#NEGTIVE ACCURACY\n",
        "\n",
        "for i in range(10):\n",
        "  T=84-8*i\n",
        "  acp,acn=ROC(T)\n",
        "  h_test_list.append(acp)\n",
        "  n_test_list.append(acn)\n",
        "  \n",
        "  \n",
        "for i in range(25):\n",
        "  T=4-0.1*i#   tune T for weigheed loss function in NN\n",
        "  acp,acn=ROC(T)\n",
        "  h_test_list.append(acp)\n",
        "  n_test_list.append(acn)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAlrkFI0kpVd"
      },
      "source": [
        "for i in range(15):\n",
        "  T=1.5-0.1*i\n",
        "  acp,acn=ROC(T)\n",
        "  h_test_list.append(acp)\n",
        "  n_test_list.append(acn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPMaboDWLGsp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "outputId": "4d0f8aa4-7021-47da-9e12-900a327ec837"
      },
      "source": [
        "plt.figure()\n",
        "plt.xlim((0, 1))\n",
        "\n",
        "plt.ylim((0, 1))\n",
        "plt.plot(n_test_list[0:50],  h_test_list[0:50])                    \n",
        "plt.scatter(h_test_list[0:50], n_test_list[0:50], marker='x')\n",
        "plt.ylabel('posive_acc')\n",
        "plt.xlabel('negtive_acc')\n",
        "\n",
        "plt.show()\n",
        "#roc plot"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAELCAYAAADHksFtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niUJERFqlRCEiIq1SohARkVb9f2AboywqRpOJAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30tqiWDfGKZp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "32e0a4f8-fb2c-4f20-d285-ba3aa1e36638"
      },
      "source": [
        "plt.savefig('fig_cat.png')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BzKKSUWp9rC"
      },
      "source": [
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "   \n",
        "    # Loop over epochs\n",
        "    for e in range(epochs):\n",
        "        \n",
        "        # Loop over batches\n",
        "        \n",
        "           \n",
        "            #loss, _  =sess.run([cost, optimizer], feed_dict={inputs_ : X_train, labels_: label, feature_: feature, tf_is_training: True})\n",
        "            train,_,_,_=sess.run([accuracy, optimizer,learning_rate,global_step], feed_dict={inputs_ :  X_tr, labels_: lab_tr})\n",
        "                \n",
        "            #, feature_:featuretrain\n",
        "            # Print at each 5 iters\n",
        "            \n",
        "                \n",
        "                #acc=sess.run(accuracy, feed_dict={inputs_ : X_train, labels_: label, feature_: feature, tf_is_training: False})\n",
        "    acp,acn,acctest=sess.run([accp,accn,accuracy], feed_dict={inputs_ : X_vld, labels_: lab_vld})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeNgKn_CHLOY"
      },
      "source": [
        "ACP,ACN=ROC(4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiWfcNvwH2ZP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "01da9a50-3fad-4432-e5d9-8ff888317c83"
      },
      "source": [
        "print(ACP)\n",
        "print(ACN)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9952\n",
            "0.16228992\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKuoBrTDII0G"
      },
      "source": [
        "h_test_list=[]\n",
        "h_test_list.append(ACP)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}